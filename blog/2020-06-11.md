title='The Noble Sevenfold Path'
subtitle='with apologies to the enlightened'
tags=['kubernetes', 'raspberry pi', 'ops', 'kubeadm']
::===::
## note
This post is **ongoing.** Check back for more stuff occasionally, or don't.

> During the release process, we publish all debs to the following repository: apt.kubernetes.io --> https://packages.cloud.google.com/apt/dists/kubernetes-xenial
> 
> In installation instructions like kubeadm, you will see references to `kubernetes-xenial`, though the packages contained therein are known to work for distributor-supported versions of Debian/Ubuntu.
>
> I'll assign myself here to document that idiosyncrasy.

~ <https://github.com/kubernetes/release/issues/728#issuecomment-563038052>

We're in for a *ride*, aren't we?

## what are you doing _now_?
I'm setting up a Kubernetes cluster, because my current job has a bizzarely high amount
of k8s in it and I should probably learn how to use it anyway. My distaste for containers
is still very real, but I've hit the point where I need _something_ to help manage the
various services and applications I want running, and k8s is The Current and Eternal
Hotness, so here we are.

### what services?
- [Lopez](https://github.com/c-x-berger/lopez) (public memo to self: redo Lopez to be
less painful)
- [ttauthority.xyz](https://ttauthority.xyz)
- [linx.ttauthority.xyz](https://linx.ttauthority.xyz)
- My Lounge instance
- A Minecraft Forge server
- The Purdue LUG Gitea
- The (brand new) family recipe book

Some of these live on machines that are Very Far Away, or don't exist at all. I'll likely
use a WireGuard VPN to get those brought into the fold.

## day zero - set up the hardware
Since I am being as cheap as possible (every cent of my pay needs to go to tuition, yay),
I'm using whatever I already have for hardware.

So my control plane is going to be a Raspberry Pi 3b, with a comparatively beefy Dell
Inspiron I had lying around for my single worker node. For a single node cluster, I could
use [MicroK8s][microk8s], but that wouldn't be half as fun.

For networking, I have a Netgear WNDR 3700v1 running OpenWRT 19.07. Setting this up was
the hardest part, as my workstation doesn't have an Ethernet port. (This is due to
coronavirus -- there are two workstations in my house's basement with Ethernet and my
parents have occupied both of them.) Thankfully, I found a guide for
["AP + STA mode"][ap-sta], which has solved the issue and given me local IPv6 as a free
bonus (my ISP's equipment does not. Shame, Smithville Fiber, shame.)

All the "compute" (control plane, worker) is going to be running ~~Raspbian~~ Raspberry
Pi OS or Debian stable.

## day one - the control plane
### step 0 - install Docker
```sh
curl -sSL https://get.docker.com | sh
```

Cool. Also, something something [cgroup something][cgroup]. Nothing hard. Yay systemd.

### step 1 - install kubeadm
`kubeadm` is a bit of software that is essentially responsible for connecting all the
infrastructure together. Actually providing the infrastructure is just a manual effort,
but on cloud platforms I hear some tools do this too. ([grumble grumble][cloud-grumble])

We'll just worry about setting it up on the Raspberry Pi today.

The [documentation][kubeadm-install] is pretty good, until you get to the actual "install
the DEB" part of things and see this:
```sh
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
```
Hey, what's `xenial` doing in my Debian and modern Ubuntu versions? Surely I can use
something else?

It turns out, no. You can try to use, say, `kubernetes-jessie`, but `apt` will spit weird
404 errors at you if you try that. Some GitHub searching later turns up that it's just
broken and fixing it is apparently not the highest priority. The can has [officially been
kicked down the road][kick-can] to 1.19, so don't hold your breath. `kubernetes-xenial`
it is...

Anyway. Just follow the instructions, turn off swap, and away we go. Time to `kubeadm
init` and celebrate, right? Nope! Enter...

### step 2 - select your ~~car~~ network
Kubernetes solves some networking problems (namely, port conflicts) by giving every Pod
its own unique IP address. Unfortunately, it doesn't seem to actually ship with a
mechanism to do this, delegating the problem to someone else.

So you need something to hand out IPs, or otherwise provide a Level 3 network. I don't
know exact details, beyond "I need to go get A Thing so Pods can talk to each other."
Where this goes from "minor annoyance" to "serious flaw" is in how this is documented.
Choosing a Pod network is a single list item in a list of otherwise optional steps. It's
not even the first or last item, it's right in the middle. Actual discussion on Pod
networks comes _after_ you've invoked `kubeadm init` and effectively made your choice.
Not only that, the descriptions of each Pod network you might consider range from "click
here for more details" to a list of supported architechtures and pre-install requirements.

I ended up going with flannel, since it was recommended for lightweight environments like
my Raspberry Pi control plane.

Somewhere around here, a more intelligent systems administrator would learn of systems
such as [k3s][] which -- despite being "built for IoT & Edge" -- include these batteries.

### step 3 - push the big red button
Now that we've figured out what our Pod network will require from us, we can finally run
`kubeadm init` and get something started:
```sh
# initialize control node
# openwrt provides a DNS server that resolves hostname.lan, let's use it. we'll probably
# never make this a HA setup anyway

# pod-network-cidr is **not** arbitrary, but the default value given in flannel's
# manifest. you can probably use something else by editing said manifest.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint raspberrypi.lan

# copy config into ~/.kube so kubectl works
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# set up flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```
Alright. Control plane is done. The good news is that was the hardest part.

## day two - the worker node
You might be wondering what's with the whole "days" thing. I have a job that takes up
enough time being near my personal computer that I want to get away from that space after
work for long enough to not leave the multiple hours available to do this all at once.

Back to main content.

Thankfully, the worker node has significantly less setup involved than the control plane,
since the control plane can just tell the worker node how to set itself up. We just
repeat the Docker setup from before, install `kubeadm`, and then join the cluster with
`kubeadm join`:
```sh
# upon running kubeadm init earlier, you should have gotten the real values to use here
# if you didn't save those, or it's been more than 24 hours, you can make a new one with
# `kubeadm token create --print-join-command`
sudo kubeadm join raspberrypi.lan:6443 --token foo.bar --discovery-token-ca-cert-hash sha265:b9f80310094947ecdba124a74222c735a8c97bf207f840e94134cf4be59b6206
```
We can now check that our cluster is online and has two nodes with `kubectl get nodes`:
```
NAME              STATUS   ROLES    AGE    VERSION
dynamic-entropy   Ready    <none>   172m   v1.18.3
raspberrypi       Ready    master   23h    v1.18.3
```
Of course, we could add as many workers as we wanted, but this _does_ constitute a 
multiple node cluster, which is most of what I needed to start with.

I also went ahead and copied the `.kube/config` file to my laptop to run `kubectl`
operations without an `ssh` session.

[microk8s]: https://microk8s.io/
[ap-sta]: https://openwrt.org/docs/guide-user/network/wifi/ap_sta/
[cgroup]: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers
[cloud-grumble]: https://rachelbythebay.com/w/2020/05/06/scale/
[kubeadm-install]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
[kick-can]: https://github.com/kubernetes/release/issues/728#issuecomment-627757733
[k3s]: https://k3s.io/
